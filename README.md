The database contains orthophoto and point cloud data from mango orchards in two different environments, in addition to methods for extracting orchard canopies using deep learning and machine learning and associated additional data

To help you make better use of this data, we have annotated the data in the repository and how to use it. Details are given below:

1.Data presentation:
(1)We have uploaded the orthophotos and point cloud data of the two orchards to the Zenodo database. Please click this link to access them. https://doi.org/10.5281/zenodo.14559873

(2)In the folder named "study_area_labeling", we provide the shapefiles (.shp) for the training and testing areas of the two orchards used in our study. After downloading the original data, you can open the data using remote sensing software (e.g., ENVI, ArcGIS, ArcGIS Pro) and import our shapefiles to crop the relevant areas of the original orthophotos. If the shapefiles and orthophotos do not align in the same layer, it is likely that the coordinate system of the orthophotos needs to be defined. You can use the Reproject Raster function in ENVI to define the coordinate system. To match the shapefiles, please set the coordinate system to "WGS_1984_UTM_Zone_49N."

(3)In the folder named "elevation data"ï¼Œ we provide the DSM, DTM, and CHM of the testing areas in the two orchards. You can overlay these datasets with the RGB images to better extract the effective features of the target objects. If you wish to obtain elevation data for other areas or the entire orchards, you can process the provided point cloud data using software such as LiDAR360, CloudCompare, or Pix4D. Since the resolution of the orthophotos we provide is approximately 0.05 m, please ensure the elevation data matches this resolution when overlaying it with the orthophotos to avoid a decline in the accuracy of the overlaid images.

(4)In the folder named "3," we provide the data labels used in our study for deep learning and machine learning in the two orchards. The data labels are saved in SHP file format. Specifically, the deep learning training labels for Orchard 1 are contained in the file Train_label_1, and the testing labels are in the file Test_label_1. For Orchard 2, the training labels are in the files Train_label_2 and Train_label_2(2), and the testing labels are in the file Test_label_2. You can open these SHP files alongside the original orthophotos in ArcGIS Pro and use the "Export Training Data for Deep Learning" tool to slice the original images and labels, generating data suitable for deep learning training and testing. It is important to note that the orthophotos generated by the UAV contain four bands, with the fourth band being unnecessary. Before slicing the images, please remove the fourth band and retain only the R, G, and B bands. For the machine learning SHP labels, please open them in ENVI and use the machine learning image classification functionality for subsequent training and classification.

2.Related Code:
(1)In our study, the codes for the four deep learning networks we used (U-Net, PSP-Net, HR-Net, and DeepLabV3+) can be found in the link we provided. This repository includes detailed annotations on how to train and test each of the four models. https://github.com/bubbliiiing/deeplabv3-plus-pytorch.

(2)The label images (.png) obtained using ArcGIS Pro have a bit depth of 16 bits, which need to be converted to 8 bits. Additionally, the output value for tree crown labels is 5, which needs to be converted to background as 0 and tree crown as 1. We provide two codes, 1.py and 2.py, to process the label images accordingly.

(3)We provide the script 3.py to evaluate the testing accuracy of the models.
